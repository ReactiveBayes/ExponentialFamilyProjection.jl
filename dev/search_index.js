var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = ExponentialFamilyProjection","category":"page"},{"location":"#ExponentialFamilyProjection","page":"Home","title":"ExponentialFamilyProjection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ExponentialFamilyProjection.jl package offers a suite of functions for projecting an arbitrary (un-normalized) log probability density function onto a specified member of the exponential family (e.g., Gaussian, Beta, Bernoulli). This is achieved by optimizing the natural parameters of the exponential family member within a defined manifold. The library leverages Manopt.jl for optimization and utilizes ExponentialFamilyManifolds.jl to define the manifolds corresponding to the members of the exponential family.","category":"page"},{"location":"#Projection-parameters","page":"Home","title":"Projection parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In order to project a log probability density function onto a member of the exponential family, the user first needs to specify projection parameters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.ProjectionParameters\nExponentialFamilyProjection.DefaultProjectionParameters\nExponentialFamilyProjection.getinitialpoint","category":"page"},{"location":"#ExponentialFamilyProjection.ProjectionParameters","page":"Home","title":"ExponentialFamilyProjection.ProjectionParameters","text":"ProjectionParameters(; kwargs...)\n\nA type to hold the parameters for the projection procedure.  The following parameters are available:\n\nstrategy = ExponentialFamilyProjection.DefaultStrategy(): The strategy to use to compute the gradients.\nniterations = 100: The number of iterations for the optimization procedure.\ntolerance = 1e-6: The tolerance for the norm of the gradient.\nstepsize = ConstantStepsize(0.1): The stepsize for the optimization procedure. Accepts stepsizes from Manopt.jl.\nseed: Optional; Seed for the rng\nrng: Optional; Random number generator\ndirection = BoundedNormUpdateRule(static(1.0): Direction update rule. Accepts Manopt.DirectionUpdateRule from Manopt.jl.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.DefaultProjectionParameters","page":"Home","title":"ExponentialFamilyProjection.DefaultProjectionParameters","text":"DefaultProjectionParameters()\n\nReturn the default parameters for the projection procedure.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.getinitialpoint","page":"Home","title":"ExponentialFamilyProjection.getinitialpoint","text":"getinitialpoint(strategy, M::AbstractManifold, parameters::ProjectionParameters)\n\nReturns an initial point to start optimization from. By default returns a rand point from M,  but different strategies may implement their own methods.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"Read more about different optimization strategies here.","category":"page"},{"location":"#Projection-family","page":"Home","title":"Projection family","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"After the parameters have been specified the user can proceed with specifying the projection type (exponential family member), its dimensionality and (optionally) the conditioner.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.ProjectedTo","category":"page"},{"location":"#ExponentialFamilyProjection.ProjectedTo","page":"Home","title":"ExponentialFamilyProjection.ProjectedTo","text":"ProjectedTo(::Type{T}, dims...; conditioner = nothing, parameters = DefaultProjectionParameters)\n\nA specification of a projection to an exponential family distribution.\n\nThe following arguments are required:\n\nType{T}: a type of an exponential family member to project to, e.g. Beta\ndims...: dimensions of the distribution, e.g. 2 for MvNormal\n\nThe following arguments are optional:\n\nconditioner = nothing: a conditioner to use for the projection, not all exponential family members require a conditioner, but some do, e.g. Laplace\nparameters = DefaultProjectionParameters: parameters for the projection procedure\n\njulia> using ExponentialFamily\n\njulia> projected_to = ProjectedTo(Beta)\nProjectedTo(Beta)\n\njulia> projected_to = ProjectedTo(Beta, parameters = ProjectionParameters(niterations = 10))\nProjectedTo(Beta)\n\njulia> projected_to = ProjectedTo(MvNormalMeanCovariance, 2)\nProjectedTo(MvNormalMeanCovariance, dims = 2)\n\njulia> projected_to = ProjectedTo(Laplace, conditioner = 2.0)\nProjectedTo(Laplace, conditioner = 2.0)\n\n\n\n\n\n","category":"type"},{"location":"#Projection","page":"Home","title":"Projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The projection is performed by calling the project_to function with the specified ExponentialFamilyProjection.ProjectedTo and log probability density function or a set of data point as the second argument.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.project_to","category":"page"},{"location":"#ExponentialFamilyProjection.project_to","page":"Home","title":"ExponentialFamilyProjection.project_to","text":"project_to(to::ProjectedTo, argument::F, supplementary..., initialpoint, kwargs...)\n\nFinds the closest projection of argument onto the exponential family distribution specified by to.\n\nArguments\n\nto::ProjectedTo: Configuration for the projection. Refer to ProjectedTo for detailed information.\nargument::F: An (un-normalized) function representing the log-PDF of an arbitrary distribution or a list of samples.\nsupplementary...: Additional distributions to project the product of argument and these distributions (optional).\ninitialpoint: Starting point for the optimization process (optional).\nkwargs...: Additional arguments passed to Manopt.gradient_descent! (optional). For details on gradient_descent! parameters, see the Manopt.jl documentation.\n\nSupplementary\n\nThe supplementary distributions must match the type and conditioner of the target distribution specified in to.  Including supplementary distributions is equivalent to modified argument function as follows:\n\nf_modified = (x) -> argument(x) + logpdf(supplementary[1], x) + logpdf(supplementary[2], x) + ...\n\njulia> using ExponentialFamily, BayesBase\n\njulia> f = (x) -> logpdf(Beta(30.14, 2.71), x);\n\njulia> prj = ProjectedTo(Beta; parameters = ProjectionParameters(niterations = 500))\nProjectedTo(Beta)\n\njulia> project_to(prj, f) isa ExponentialFamily.Beta\ntrue\n\njulia> using ExponentialFamily, BayesBase, StableRNGs\n\njulia> samples = rand(StableRNG(42), Beta(30.14, 2.71), 1_000);\n\njulia> prj = ProjectedTo(Beta; parameters = ProjectionParameters(tolerance = 1e-2))\nProjectedTo(Beta)\n\njulia> project_to(prj, samples) isa ExponentialFamily.Beta\ntrue\n\nnote: Note\nDifferent strategies are compatible with different types of arguments. Read optimization strategies section in the documentation for more information.\n\n\n\n\n\n","category":"function"},{"location":"#opt-strategies","page":"Home","title":"Optimization strategies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The optimization procedure requires computing the expectation of the gradient to perform gradient descent in the natural parameters space. Currently, the library provides the following strategies for computing these expectations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.DefaultStrategy\nExponentialFamilyProjection.ControlVariateStrategy\nExponentialFamilyProjection.MLEStrategy\nExponentialFamilyProjection.preprocess_strategy_argument\nExponentialFamilyProjection.create_state!\nExponentialFamilyProjection.prepare_state!\nExponentialFamilyProjection.compute_cost\nExponentialFamilyProjection.compute_gradient!","category":"page"},{"location":"#ExponentialFamilyProjection.DefaultStrategy","page":"Home","title":"ExponentialFamilyProjection.DefaultStrategy","text":"DefaultStrategy\n\nThe DefaultStrategy selects the optimal projection strategy based on the type of the second argument provided to the project_to function.\n\nRules:\n\nIf the second argument is an AbstractArray, use MLEStrategy.\nFor all other types, use ControlVariateStrategy.\n\nnote: Note\nThe rules above are subject to change.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.ControlVariateStrategy","page":"Home","title":"ExponentialFamilyProjection.ControlVariateStrategy","text":"ControlVariateStrategy(; kwargs...)\n\nA strategy for gradient descent optimization and gradients computations that resembles the REINFORCE gradient estimator.\n\nThe following parameters are available:\n\nnsamples = 2000: The number of samples to use for estimates\nbuffer = StaticTools.MallocSlabBuffer(): Advanced option; A buffer for temporary computations\n\nnote: Note\nThis strategy requires a function as an argument for project_to and cannot project a collection of samples. Use MLEStrategy to project a collection of samples.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.MLEStrategy","page":"Home","title":"ExponentialFamilyProjection.MLEStrategy","text":"MLEStrategy()\n\nA strategy for gradient descent optimization and gradients computations that resembles MLE estimation.\n\nnote: Note\nThis strategy requires a collection of samples as an argument for project_to and cannot project a function. Use ControlVariateStrategy to project a function.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.preprocess_strategy_argument","page":"Home","title":"ExponentialFamilyProjection.preprocess_strategy_argument","text":"preprocess_strategy_argument(strategy, argument)\n\nChecks the compatibility of strategy with argument and returns a modified strategy and argument if needed.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.create_state!","page":"Home","title":"ExponentialFamilyProjection.create_state!","text":"create_state!(\n    strategy,\n    M::AbstractManifold,\n    parameters::ProjectionParameters,\n    projection_argument,\n    initial_ef,\n    supplementary_η,\n)\n\nCreates, initializes and returns a state for the strategy with the given parameters.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.prepare_state!","page":"Home","title":"ExponentialFamilyProjection.prepare_state!","text":"prepare_state!(\n    strategy,\n    state,\n    M::AbstractManifold,\n    parameters::ProjectionParameters,\n    projection_argument,\n    distribution,\n    supplementary_η,\n)\n\nPrepares an existing state of the strategy for the new optimization iteration for use by setting or updating its internal parameters.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.compute_cost","page":"Home","title":"ExponentialFamilyProjection.compute_cost","text":"compute_cost(\n    M::AbstractManifold,\n    strategy,\n    state,\n    η,\n    logpartition,\n    gradlogpartition,\n    inv_fisher,\n)\n\nCompute the cost using the provided strategy.\n\nArguments\n\nM::AbstractManifold: The manifold on which the computations are performed.\nstrategy: The strategy used for computation of the cost value.\nstate: The current state for the strategy.\nη: Parameter vector.\nlogpartition: The log partition of the current point (η).\ngradlogpartition: The gradient of the log partition of the current point (η).\ninv_fisher: The inverse Fisher information matrix of the current point (η).\n\nReturns\n\ncost: The computed cost value.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.compute_gradient!","page":"Home","title":"ExponentialFamilyProjection.compute_gradient!","text":"compute_gradient!(\n    M::AbstractManifold,\n    strategy,\n    state,\n    X,\n    η,\n    logpartition,\n    gradlogpartition,\n    inv_fisher,\n)\n\nUpdates the gradient X in-place using the provided strategy.\n\nArguments\n\nM::AbstractManifold: The manifold on which the computations are performed.\nstrategy: The strategy used for computation of the gradient value.\nstate: The current state of the control variate strategy.\nX: The storage for the gradient.\nη: Parameter vector.\nlogpartition: The log partition of the current point (η).\ngradlogpartition: The gradient of the log partition of the current point (η).\ninv_fisher: The inverse Fisher information matrix of the current point (η).\n\nReturns\n\nX: The computed gradient (updated in-place)\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"For high-dimensional distributions, adjusting the default number of samples might be necessary to achieve better performance.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"#Gaussian-projection","page":"Home","title":"Gaussian projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In this example we project an arbitrary log probability density function onto a Gaussian distribution. The log probability density function is defined using another Gaussian, but it can be any function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ExponentialFamilyProjection, ExponentialFamily, BayesBase\nusing Test #hide\nusing Distributions #hide\n\nhiddengaussian = NormalMeanVariance(3.14, 2.71)\ntargetf = (x) -> logpdf(hiddengaussian, x)\nprj = ProjectedTo(NormalMeanVariance)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddengaussian) < 1e-3 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can see that the estimated result is pretty close to the actual hiddengaussian used to define the targetf. We can also visualise the results using the Plots.jl package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Plots\n\nplot(-6.0:0.1:12.0, x -> pdf(hiddengaussian, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nplot!(-6.0:0.1:12.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's also try to project an arbitrary unnormalized log probability density function onto a Gaussian distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# `+ 100` to ensure that the function is unnormalized\ntargetf = (x) -> -0.5 * (x - 3.14)^2 + 100\n\nresult = project_to(prj, targetf)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In this case, targetf does not define any valid probability distribution since it is unnormalized, but the project_to function is able to project it onto a closest possible Gaussian distribution. We can again visualize the results using the Plots.jl package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(-40.0:0.1:40.0, targetf, label=\"unnormalized logpdf\", fill = 0, fillalpha = 0.2)\nplot!(-40.0:0.1:40.0, (x) -> logpdf(result, x), label=\"estimated logpdf of a Gaussian\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Beta-projection","page":"Home","title":"Beta projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The experiment can be performed for other members of the exponential family as well. For example, let's project an arbitrary log probability density function onto a Beta distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"hiddenbeta = Beta(10, 3)\ntargetf = (x) -> logpdf(hiddenbeta, x)\nprj = ProjectedTo(Beta)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddenbeta) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"And let's visualize the result using the Plots.jl package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(0.0:0.01:1.0, x -> pdf(hiddenbeta, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nplot!(0.0:0.01:1.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Multivariate-Gaussian-projection","page":"Home","title":"Multivariate Gaussian projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The library also supports multivariate distributions. Let's project an arbitrary log probability density function onto a multivariate Gaussian distribution.","category":"page"},{"location":"","page":"Home","title":"Home","text":"hiddengaussian = MvNormalMeanCovariance(\n    [ 3.14, 2.17 ],\n    [ 2.0 -0.1; -0.1 3.0 ]\n)\ntargetf = (x) -> logpdf(hiddengaussian, x)\nprj = ProjectedTo(MvNormalMeanCovariance, 2)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddengaussian) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"As in previous examples the result is pretty close to the actual hiddengaussian used to define the targetf. ","category":"page"},{"location":"#Projection-with-samples","page":"Home","title":"Projection with samples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The projection can be done given a set of samples instead of the function directly. For example, let's project an set of samples onto a Beta distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StableRNGs\n\nhiddenbeta = Beta(10, 3)\nsamples = rand(StableRNG(42), hiddenbeta, 1_000)\nprj = ProjectedTo(Beta)\nresult = project_to(prj, samples)\n@test kldivergence(result, hiddenbeta) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(0.0:0.01:1.0, x -> pdf(hiddenbeta, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nhistogram!(samples, label = \"samples\", normalize = :pdf, fillalpha = 0.2)\nplot!(0.0:0.01:1.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Manopt-extensions","page":"Home","title":"Manopt extensions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.ProjectionCostGradientObjective","category":"page"},{"location":"#ExponentialFamilyProjection.ProjectionCostGradientObjective","page":"Home","title":"ExponentialFamilyProjection.ProjectionCostGradientObjective","text":"ProjectionCostGradientObjective\n\nThis structure provides an interface for Manopt to compute the cost and gradients required for the optimization procedure based on manifold projection. The actual computation of costs and gradients is defined by the strategy argument.\n\nArguments\n\nprojection_parameters: The parameters for projection, must be of type ProjectionParameters\nprojection_argument: The second argument of the project_to function.\ncurrent_η: Current optimization point.\nsupplementary_η: A tuple of additional natural parameters subtracted from the current point in each optimization iteration.\nstrategy: Specifies the method for computing costs and gradients, which may support different projection_argument values.\nstrategy_state: The state for the strategy, usually created with create_state!\n\nnote: Note\nThis structure is internal and is subject to change.\n\n\n\n\n\n","category":"type"},{"location":"#Bounded-direction-update-rule","page":"Home","title":"Bounded direction update rule","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ExponentialFamilyProjection.jl package implements a specialized gradient direction rule that limits the norm (manifold-specific) of the gradient to a pre-specified value.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExponentialFamilyProjection.BoundedNormUpdateRule","category":"page"},{"location":"#ExponentialFamilyProjection.BoundedNormUpdateRule","page":"Home","title":"ExponentialFamilyProjection.BoundedNormUpdateRule","text":"BoundedNormUpdateRule(limit; direction = IdentityUpdateRule())\n\nA DirectionUpdateRule is a direction rule that constrains the norm of the direction to a specified limit.\n\nThis rule operates in two steps:\n\nInitial direction computation: It first applies the specified direction update rule to compute an initial direction.\nNorm check and scaling: The norm of the resulting direction vector is checked using Manopt.norm(M, p, d), where:\nM` is the manifold on which the optimization is running,\np is the point at which the direction was computed,\nd is the computed direction.\nIf this norm exceeds the specified limit, the direction vector is scaled down so that its new norm exactly equals the limit. This scaling preserves the direction of the gradient while controlling its magnitude.\n\nRead more about Manopt.DirectionUpdateRule in the Manopt.jl documentation.\n\n\n\n\n\n","category":"type"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
