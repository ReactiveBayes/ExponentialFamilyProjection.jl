var documenterSearchIndex = {"docs":
[{"location":"#ExponentialFamilyProjection","page":"Home","title":"ExponentialFamilyProjection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ExponentialFamilyProjection.jl package offers a suite of functions for projecting an arbitrary (un-normalized) log probability density function onto a specified member of the exponential family (e.g., Gaussian, Beta, Bernoulli). This is achieved by optimizing the natural parameters of the exponential family member within a defined manifold. The library leverages Manopt.jl for optimization and utilizes ExponentialFamilyManifolds.jl to define the manifolds corresponding to the members of the exponential family.","category":"page"},{"location":"#Projection-parameters","page":"Home","title":"Projection parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In order to project a log probability density function onto a member of the exponential family, the user first needs to specify projection parameters:","category":"page"},{"location":"#ExponentialFamilyProjection.ProjectionParameters","page":"Home","title":"ExponentialFamilyProjection.ProjectionParameters","text":"ProjectionParameters(; kwargs...)\n\nA type to hold the parameters for the projection procedure.  The following parameters are available:\n\nstrategy = ExponentialFamilyProjection.DefaultStrategy(): The strategy to use to compute the gradients.\nniterations = 100: The number of iterations for the optimization procedure.\ntolerance = 1e-6: The tolerance for the norm of the gradient.\nstepsize = ConstantLength(0.1): The stepsize for the optimization procedure. Accepts stepsizes from Manopt.jl.\nseed: Optional; Seed for the rng\nrng: Optional; Random number generator\ndirection = BoundedNormUpdateRule(static(1.0): Direction update rule. Accepts Manopt.DirectionUpdateRule from Manopt.jl.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.DefaultProjectionParameters","page":"Home","title":"ExponentialFamilyProjection.DefaultProjectionParameters","text":"DefaultProjectionParameters()\n\nReturn the default parameters for the projection procedure.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.getinitialpoint","page":"Home","title":"ExponentialFamilyProjection.getinitialpoint","text":"getinitialpoint(strategy, M::AbstractManifold, parameters::ProjectionParameters)\n\nReturns an initial point to start optimization from. By default returns a rand point from M,  but different strategies may implement their own methods.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"Read more about different optimization strategies here.","category":"page"},{"location":"#Projection-family","page":"Home","title":"Projection family","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"After the parameters have been specified the user can proceed with specifying the projection type (exponential family member), its dimensionality and (optionally) the conditioner.","category":"page"},{"location":"#ExponentialFamilyProjection.ProjectedTo","page":"Home","title":"ExponentialFamilyProjection.ProjectedTo","text":"ProjectedTo(::Type{T}, dims...; conditioner = nothing, parameters = DefaultProjectionParameters)\n\nA specification of a projection to an exponential family distribution.\n\nThe following arguments are required:\n\nType{T}: a type of an exponential family member to project to, e.g. Beta\ndims...: dimensions of the distribution, e.g. 2 for MvNormal\n\nThe following arguments are optional:\n\nconditioner = nothing: a conditioner to use for the projection, not all exponential family members require a conditioner, but some do, e.g. Laplace\nparameters = DefaultProjectionParameters: parameters for the projection procedure\nkwargs = nothing: Additional arguments passed to Manopt.gradient_descent! (optional). For details on gradient_descent! parameters, see the Manopt.jl documentation. Note, that kwargs passed to project_to take precedence over kwargs specified in the parameters.\n\njulia> using ExponentialFamily\n\njulia> projected_to = ProjectedTo(Beta)\nProjectedTo(Beta)\n\njulia> projected_to = ProjectedTo(Beta, parameters = ProjectionParameters(niterations = 10))\nProjectedTo(Beta)\n\njulia> projected_to = ProjectedTo(MvNormalMeanCovariance, 2)\nProjectedTo(MvNormalMeanCovariance, dims = 2)\n\njulia> projected_to = ProjectedTo(Laplace, conditioner = 2.0)\nProjectedTo(Laplace, conditioner = 2.0)\n\n\n\n\n\n","category":"type"},{"location":"#Projection","page":"Home","title":"Projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The projection is performed by calling the project_to function with the specified ExponentialFamilyProjection.ProjectedTo and log probability density function or a set of data point as the second argument.","category":"page"},{"location":"#ExponentialFamilyProjection.project_to","page":"Home","title":"ExponentialFamilyProjection.project_to","text":"project_to(to::ProjectedTo, argument::F, supplementary..., initialpoint, kwargs...)\n\nFinds the closest projection of argument onto the exponential family distribution specified by to.\n\nArguments\n\nto::ProjectedTo: Configuration for the projection. Refer to ProjectedTo for detailed information.\nargument::F: An (un-normalized) function representing the log-PDF of an arbitrary distribution or a list of samples.\nsupplementary...: Additional distributions to project the product of argument and these distributions (optional).\ninitialpoint: Starting point for the optimization process (optional).\nkwargs...: Additional arguments passed to Manopt.gradient_descent! (optional). For details on gradient_descent! parameters, see the Manopt.jl documentation.\n\nSupplementary\n\nThe supplementary distributions must match the type and conditioner of the target distribution specified in to.  Including supplementary distributions is equivalent to modified argument function as follows:\n\nf_modified = (x) -> argument(x) + logpdf(supplementary[1], x) + logpdf(supplementary[2], x) + ...\n\njulia> using ExponentialFamily, BayesBase\n\njulia> f = (x) -> logpdf(Beta(30.14, 2.71), x);\n\njulia> prj = ProjectedTo(Beta; parameters = ProjectionParameters(niterations = 500))\nProjectedTo(Beta)\n\njulia> project_to(prj, f) isa ExponentialFamily.Beta\ntrue\n\njulia> using ExponentialFamily, BayesBase, StableRNGs\n\njulia> samples = rand(StableRNG(42), Beta(30.14, 2.71), 1_000);\n\njulia> prj = ProjectedTo(Beta; parameters = ProjectionParameters(tolerance = 1e-2))\nProjectedTo(Beta)\n\njulia> project_to(prj, samples) isa ExponentialFamily.Beta\ntrue\n\nnote: Note\nDifferent strategies are compatible with different types of arguments. Read optimization strategies section in the documentation for more information.\n\n\n\n\n\n","category":"function"},{"location":"#opt-strategies","page":"Home","title":"Optimization strategies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The optimization procedure requires computing the expectation of the gradient to perform gradient descent in the natural parameters space. Currently, the library provides the following strategies for computing these expectations:","category":"page"},{"location":"#ExponentialFamilyProjection.DefaultStrategy","page":"Home","title":"ExponentialFamilyProjection.DefaultStrategy","text":"DefaultStrategy\n\nThe DefaultStrategy selects the optimal projection strategy based on the type of the second argument provided to the project_to function.\n\nRules:\n\nIf the second argument is an AbstractArray, use MLEStrategy.\nFor all other types, use ControlVariateStrategy.\n\nnote: Note\nThe rules above are subject to change.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.ControlVariateStrategy","page":"Home","title":"ExponentialFamilyProjection.ControlVariateStrategy","text":"ControlVariateStrategy(; kwargs...)\n\nA strategy for gradient descent optimization and gradients computations that resembles the REINFORCE gradient estimator.\n\nThe following parameters are available:\n\nnsamples = 2000: The number of samples to use for estimates\nbuffer = Bumper.SlabBuffer(): Advanced option; A buffer for temporary computations\n\nnote: Note\nThis strategy requires a function as an argument for project_to and cannot project a collection of samples. Use MLEStrategy to project a collection of samples.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.MLEStrategy","page":"Home","title":"ExponentialFamilyProjection.MLEStrategy","text":"MLEStrategy()\n\nA strategy for gradient descent optimization and gradients computations that resembles MLE estimation.\n\nnote: Note\nThis strategy requires a collection of samples as an argument for project_to and cannot project a function. Use ControlVariateStrategy to project a function.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.BonnetStrategy","page":"Home","title":"ExponentialFamilyProjection.BonnetStrategy","text":"BonnetStrategy{S, TL}\n\nA strategy for gradient descent optimization and gradients computations that resembles the Bonnet gradient estimator that works for normal distributions. It's based on the equations (10) and (11) in Khan, 2024.\n\nThe following parameters are available:\n\nnsamples = 2000: The number of samples to use for estimates\n\nnote: Note\nThis strategy requires a function as an argument for project_to and cannot project a collection of samples. Use MLEStrategy to project a collection of samples. This strategy requires a logpdf function that can be converted to an InplaceLogpdfGradHess object. This strategy requires the normal manifold.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.GaussNewton","page":"Home","title":"ExponentialFamilyProjection.GaussNewton","text":"GaussNewton{S,TL}\n\nA deterministic strategy that resembles the Bonnet gradient with a point-mass approximation (no sampling). For normal distributions, it evaluates logpdf/grad/Hessian once at the current mean and takes a step. It implements the update akin to Eq. (13) in Khan, 2024.\n\nnote: Note\nThis strategy requires a function as an argument for project_to and cannot project a collection of samples. Use MLEStrategy to project a collection of samples. This strategy requires a logpdf function that can be converted to an InplaceLogpdfGradHess object. This strategy requires the normal manifold.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.preprocess_strategy_argument","page":"Home","title":"ExponentialFamilyProjection.preprocess_strategy_argument","text":"preprocess_strategy_argument(strategy, argument)\n\nChecks the compatibility of strategy with argument and returns a modified strategy and argument if needed.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.create_state!","page":"Home","title":"ExponentialFamilyProjection.create_state!","text":"create_state!(\n    strategy,\n    M::AbstractManifold,\n    parameters::ProjectionParameters,\n    projection_argument,\n    initial_ef,\n    supplementary_η,\n)\n\nCreates, initializes and returns a state for the strategy with the given parameters.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.prepare_state!","page":"Home","title":"ExponentialFamilyProjection.prepare_state!","text":"prepare_state!(\n    strategy,\n    state,\n    M::AbstractManifold,\n    parameters::ProjectionParameters,\n    projection_argument,\n    distribution,\n    supplementary_η,\n)\n\nPrepares an existing state of the strategy for the new optimization iteration for use by setting or updating its internal parameters.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.compute_cost","page":"Home","title":"ExponentialFamilyProjection.compute_cost","text":"compute_cost(\n    M::AbstractManifold,\n    strategy,\n    state,\n    η,\n    logpartition,\n    gradlogpartition,\n    inv_fisher,\n)\n\nCompute the cost using the provided strategy.\n\nArguments\n\nM::AbstractManifold: The manifold on which the computations are performed.\nstrategy: The strategy used for computation of the cost value.\nstate: The current state for the strategy.\nη: Parameter vector.\nlogpartition: The log partition of the current point (η).\ngradlogpartition: The gradient of the log partition of the current point (η).\ninv_fisher: The inverse Fisher information matrix of the current point (η).\n\nReturns\n\ncost: The computed cost value.\n\n\n\n\n\n","category":"function"},{"location":"#ExponentialFamilyProjection.compute_gradient!","page":"Home","title":"ExponentialFamilyProjection.compute_gradient!","text":"compute_gradient!(\n    M::AbstractManifold,\n    strategy,\n    state,\n    X,\n    η,\n    logpartition,\n    gradlogpartition,\n    inv_fisher,\n)\n\nUpdates the gradient X in-place using the provided strategy.\n\nArguments\n\nM::AbstractManifold: The manifold on which the computations are performed.\nstrategy: The strategy used for computation of the gradient value.\nstate: The current state of the control variate strategy.\nX: The storage for the gradient.\nη: Parameter vector.\nlogpartition: The log partition of the current point (η).\ngradlogpartition: The gradient of the log partition of the current point (η).\ninv_fisher: The inverse Fisher information matrix of the current point (η).\n\nReturns\n\nX: The computed gradient (updated in-place)\n\n\n\n\n\n","category":"function"},{"location":"#In-place-logpdf/grad/Hessian-adapters","page":"Home","title":"In-place logpdf/grad/Hessian adapters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The library provides convenient wrappers to evaluate log-density, gradient, and Hessian in-place, and an adapter to combine separate grad!/hess! into a single grad_hess!.","category":"page"},{"location":"#ExponentialFamilyProjection.InplaceLogpdfGradHess","page":"Home","title":"ExponentialFamilyProjection.InplaceLogpdfGradHess","text":"InplaceLogpdfGradHess(logpdf!, grad_hess!)\n\nWraps logpdf! and the unified grad_hess! function in a type used for dispatch. The unified interface evaluates gradient and Hessian together for efficiency.\n\nArguments\n\nlogpdf!: Function that takes (out_logpdf, x) and writes the logpdf to out_logpdf\ngrad_hess!: Function that takes (out_grad, out_hess, x) and writes gradient and Hessian\n\nMethods\n\nlogpdf!(structure, out, x)\ngrad_hess!(structure, out_grad, out_hess, x)\n\nAll methods expect pre-allocated containers of appropriate dimensions.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.InplaceLogpdfGradHess-Tuple{Any, Any, Any}","page":"Home","title":"ExponentialFamilyProjection.InplaceLogpdfGradHess","text":"InplaceLogpdfGradHess(logpdf!, grad!, hess!)\n\nOuter convenience constructor that accepts separate grad! and hess! functions. Internally it wraps them with NaiveGradHess to provide a unified grad_hess! implementation and returns an InplaceLogpdfGradHess instance.\n\nArguments\n\nlogpdf!: Function (out_logpdf, x) -> writes the log-density into out_logpdf\ngrad!: Function (out_grad, x) -> writes the gradient into out_grad\nhess!: Function (out_hess, x) -> writes the Hessian into out_hess\n\nSee also\n\nNaiveGradHess — adapter that combines separate grad!/hess! into grad_hess!.\n\n\n\n\n\n","category":"method"},{"location":"#ExponentialFamilyProjection.NaiveGradHess","page":"Home","title":"ExponentialFamilyProjection.NaiveGradHess","text":"NaiveGradHess{G, H}\n\nAdapter that exposes only grad_hess! by calling provided grad! and hess! sequentially. Useful as a fallback when a combined implementation is not available.\n\n\n\n\n\n","category":"type"},{"location":"#ExponentialFamilyProjection.logpdf!-Tuple{ExponentialFamilyProjection.InplaceLogpdfGradHess, Any, Any}","page":"Home","title":"ExponentialFamilyProjection.logpdf!","text":"logpdf!(inplace::InplaceLogpdfGradHess, out, x)\n\nEvaluate the log probability density function at point x, writing the result to pre-allocated container out.\n\n\n\n\n\n","category":"method"},{"location":"#ExponentialFamilyProjection.grad_hess!-Tuple{ExponentialFamilyProjection.InplaceLogpdfGradHess, Any, Any, Any}","page":"Home","title":"ExponentialFamilyProjection.grad_hess!","text":"grad_hess!(inplace::InplaceLogpdfGradHess, out_grad, out_hess, x)\n\nEvaluate the gradient and the Hessian at point x, writing the result to pre-allocated containers out_grad and out_hess.\n\n\n\n\n\n","category":"method"},{"location":"#ExponentialFamilyProjection.grad_hess!-Tuple{ExponentialFamilyProjection.NaiveGradHess, Any, Any, Any}","page":"Home","title":"ExponentialFamilyProjection.grad_hess!","text":"grad_hess!(inplace::NaiveGradHess, out_grad, out_hess, x)\n\nEvaluate the gradient and the Hessian at point x using the provided separate implementations.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"For high-dimensional distributions, adjusting the default number of samples might be necessary to achieve better performance.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"#Gaussian-projection","page":"Home","title":"Gaussian projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In this example we project an arbitrary log probability density function onto a Gaussian distribution. The log probability density function is defined using another Gaussian, but it can be any function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ExponentialFamilyProjection, ExponentialFamily, BayesBase\nusing Test #hide\nusing Distributions #hide\n\nhiddengaussian = NormalMeanVariance(3.14, 2.71)\ntargetf = (x) -> logpdf(hiddengaussian, x)\nprj = ProjectedTo(NormalMeanVariance)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddengaussian) < 1e-3 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can see that the estimated result is pretty close to the actual hiddengaussian used to define the targetf. We can also visualise the results using the Plots.jl package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Plots\n\nplot(-6.0:0.1:12.0, x -> pdf(hiddengaussian, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nplot!(-6.0:0.1:12.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's also try to project an arbitrary unnormalized log probability density function onto a Gaussian distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# `+ 100` to ensure that the function is unnormalized\ntargetf = (x) -> -0.5 * (x - 3.14)^2 + 100\n\nresult = project_to(prj, targetf)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In this case, targetf does not define any valid probability distribution since it is unnormalized, but the project_to function is able to project it onto a closest possible Gaussian distribution. We can again visualize the results using the Plots.jl package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(-40.0:0.1:40.0, targetf, label=\"unnormalized logpdf\", fill = 0, fillalpha = 0.2)\nplot!(-40.0:0.1:40.0, (x) -> logpdf(result, x), label=\"estimated logpdf of a Gaussian\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Beta-projection","page":"Home","title":"Beta projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The experiment can be performed for other members of the exponential family as well. For example, let's project an arbitrary log probability density function onto a Beta distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"hiddenbeta = Beta(10, 3)\ntargetf = (x) -> logpdf(hiddenbeta, x)\nprj = ProjectedTo(Beta)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddenbeta) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"And let's visualize the result using the Plots.jl package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(0.0:0.01:1.0, x -> pdf(hiddenbeta, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nplot!(0.0:0.01:1.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Multivariate-Gaussian-projection","page":"Home","title":"Multivariate Gaussian projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The library also supports multivariate distributions. Let's project an arbitrary log probability density function onto a multivariate Gaussian distribution.","category":"page"},{"location":"","page":"Home","title":"Home","text":"hiddengaussian = MvNormalMeanCovariance(\n    [ 3.14, 2.17 ],\n    [ 2.0 -0.1; -0.1 3.0 ]\n)\ntargetf = (x) -> logpdf(hiddengaussian, x)\nprj = ProjectedTo(MvNormalMeanCovariance, 2)\nresult = project_to(prj, targetf)\n@test kldivergence(result, hiddengaussian) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"As in previous examples the result is pretty close to the actual hiddengaussian used to define the targetf. ","category":"page"},{"location":"#Gauss–Newton-strategy-(logistic-regression)","page":"Home","title":"Gauss–Newton strategy (logistic regression)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Gauss–Newton strategy uses first and second derivatives of the target log-density to form a deterministic update, avoiding Monte Carlo sampling. This is useful when you can provide in-place logpdf!, grad!, and hess! for your target. Below we demonstrate projecting a Bayesian logistic regression model (which is not a normalized distribution) onto a multivariate Gaussian using Gauss–Newton strategy GaussNewton.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We split this example into small steps and use a shared example environment so that variables (including a stable RNG) persist between blocks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the following block we sample X (our features) and y (binary outputs).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LinearAlgebra\nusing StableRNGs\nusing Distributions\nusing ExponentialFamily\nusing ExponentialFamilyProjection\nusing StatsFuns\nusing Plots\n\n# 1) Generate a reproducible dataset (shared RNG)\nrng = StableRNG(42)\nn = 600\ninput_dim = 2\nd = input_dim + 1\nX_feat = randn(rng, n, input_dim)\nX = hcat(ones(n), X_feat)\nβ_true = [0.5, 2.0, -1.5]\np = map(logistic, X * β_true)\ny = rand.(Ref(rng), Bernoulli.(p));\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We created a binary logistic regression dataset with an intercept and fixed rng for reproducibility.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 2) Define in-place log-posterior, gradient, and Hessian\nfunction logpost!(out::AbstractVector{T}, β::AbstractVector{T}) where {T<:Real}\n    Xβ = X * β\n    out[1] = mean(y .* Xβ .- log.(1 .+ exp.(Xβ)))\n    return out\nend\n\nfunction grad!(out::AbstractVector{T}, β::AbstractVector{T}) where {T<:Real}\n    fill!(out, 0)\n    Xβ = X * β\n    @inbounds for i in 1:n\n        pi = 1 / (1 + exp(-Xβ[i]))\n        @views out[:] .+= (y[i] - pi) .* X[i, :]\n    end\n    out .= out ./ length(y)\n    return \nend\n\nfunction hess!(out::AbstractMatrix{T}, β::AbstractVector{T}) where {T<:Real}\n    Xβ = X * β\n    fill!(out, 0)\n    @inbounds for i in 1:n\n        pi = 1 / (1 + exp(-Xβ[i]))\n        wi = pi * (1 - pi)\n        @views out .-= wi .* (X[i, :] * transpose(X[i, :]))\n    end\n    out .= out ./ length(y)\n    return out\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"These in-place routines allow Gauss–Newton to form deterministic updates without Monte Carlo sampling.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 3) Wrap and run Gauss–Newton projection\ninplace = ExponentialFamilyProjection.InplaceLogpdfGradHess(logpost!, grad!, hess!)\nparams = ProjectionParameters(\n    tolerance = 1e-8,\n    strategy = ExponentialFamilyProjection.GaussNewton(nsamples = 1), # deterministic\n)\nprj = ProjectedTo(MvNormalMeanCovariance, d; parameters = params)\nresult = project_to(prj, inplace)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This projects the posterior to an MvNormalMeanCovariance parameterization using Gauss–Newton updates.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 4) Inspect the projection result\nμ = mean(result)\nΣ = cov(result)\nμ, size(Σ) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now we visualize the posterior-mean decision boundary and probability map. We compute a grid over feature space and evaluate the mean prediction σ(μ₀ + μ₁ x₁ + μ₂ x₂).","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 5) Build grid and compute posterior-mean probabilities\nx1_min = minimum(X[:, 2]) - 3.0\nx1_max = maximum(X[:, 2]) + 3.0\nx2_min = minimum(X[:, 3]) - 3.0\nx2_max = maximum(X[:, 3]) + 3.0\n\nxs = range(x1_min, x1_max; length = 200)\nys = range(x2_min, x2_max; length = 200)\nZ = Array{Float64}(undef, length(xs), length(ys))\nfor (i, x1) in enumerate(xs)\n    for (j, x2) in enumerate(ys)\n        z = μ[1] + μ[2] * x1 + μ[3] * x2\n        Z[i, j] = 1.0 / (1.0 + exp(-z))\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 6) Render probability heatmap and 0.5 decision contour with data overlay\nplt_mean = contourf(\n    xs, ys, Z';\n    levels = 0:0.05:1,\n    c = cgrad([:red, :green]),\n    alpha = 0.65,\n    colorbar_title = \"P(y=1)\",\n    contour_lines = false,\n    linecolor = :transparent,\n    linewidth = 0,\n    size = (650, 500),\n)\ncontour!(xs, ys, Z'; levels = [0.5], linecolor = :black, linewidth = 3, label = nothing)\nscatter!(\n    X[y .== 0, 2], X[y .== 0, 3];\n    markersize = 6,\n    markerstrokecolor = :white,\n    markerstrokewidth = 0.8,\n    label = \"y = 0\",\n    color = :red4,\n)\nscatter!(\n    X[y .== 1, 2], X[y .== 1, 3];\n    markersize = 6,\n    markerstrokecolor = :white,\n    markerstrokewidth = 0.8,\n    label = \"y = 1\",\n    color = :green4,\n)\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\ntitle!(\"mean boundary\")\nplt_mean # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"To account for parameter uncertainty, we can estimate the predictive probability by Monte Carlo: sample coefficients β from the Gaussian posterior result ~ N(μ, Σ) and average σ(β₀ + β₁ x₁ + β₂ x₂) over samples. This yields a boundary reflecting posterior spread.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 7) Monte Carlo-averaged predictive map from posterior β ~ N(μ, Σ)\nnsamples_pred = 200\nZmc = zeros(length(xs), length(ys))\nmvn_post = MvNormal(μ, Symmetric(Σ))\nfor s in 1:nsamples_pred\n    βs = rand(rng, mvn_post)\n    for (i, x1) in enumerate(xs)\n        for (j, x2) in enumerate(ys)\n            z = βs[1] + βs[2] * x1 + βs[3] * x2\n            Zmc[i, j] += 1.0 / (1.0 + exp(-z))\n        end\n    end\nend\nZmc ./= nsamples_pred\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 8) Render MC-averaged probability heatmap and decision contour\nplt_mc = contourf(\n    xs, ys, Zmc';\n    levels = 0:0.05:1,\n    c = cgrad([:red, :green]),\n    alpha = 0.65,\n    colorbar_title = \"E[P(y=1)]\",\n    contour_lines = false,\n    linecolor = :transparent,\n    linewidth = 0,\n    size = (650, 500),\n)\ncontour!(xs, ys, Zmc'; levels = [0.5], linecolor = :black, linewidth = 3, label = nothing)\nscatter!(\n    X[y .== 0, 2], X[y .== 0, 3];\n    markersize = 6,\n    markerstrokecolor = :white,\n    markerstrokewidth = 0.8,\n    label = \"y = 0\",\n    color = :red4,\n)\nscatter!(\n    X[y .== 1, 2], X[y .== 1, 3];\n    markersize = 6,\n    markerstrokecolor = :white,\n    markerstrokewidth = 0.8,\n    label = \"y = 1\",\n    color = :green4,\n)\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\ntitle!(\"full posterior boundary\")\nplt_mc","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 9) Optional: side-by-side comparison\nplot(plt_mean, plt_mc; layout = (1, 2), size = (1100, 450))","category":"page"},{"location":"#How-to-use-autograd-with-Gauss–Newton-(Enzyme.jl)","page":"Home","title":"How to use autograd with Gauss–Newton (Enzyme.jl)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You do not need to hand-derive gradients or Hessians to use Gauss–Newton. With Enzyme.jl, you can automatically obtain both and use them through the same in-place API shown above. In practice, this is typically faster and yields more stable estimates than naïve manual derivatives. Enzyme.jl has some sharp edges; please consult the Enzyme documentation before use.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Enzyme\nusing BenchmarkTools\n\n# 10) Define the log-posterior for logistic regression with a standard normal prior\nfunction obj(β::AbstractVector, X::AbstractMatrix, y::AbstractVector)\n    Xβ = X * β\n    return mean(y .* Xβ .- log.(1 .+ exp.(Xβ)))\nend\n\n# Reverse-mode gradient and forward-over-reverse Hessian via Enzyme\ngrad_enzyme = (β, X, y) -> Enzyme.gradient(Reverse, obj, β, Const(X), Const(y))[1]\nfunction jacobian_enzyme(β, X, y)\n    Enzyme.jacobian(set_runtime_activity(Forward), grad_enzyme, β, Const(X), Const(y))\nend\n\n# 11) In-place wrappers expected by Gauss–Newton\nfunction make_logpost!(X, y)\n    (out, β) -> (out[1] = obj(β, X, y); out)\nend\nfunction make_grad!(X, y)\n    function _grad!(out::AbstractVector{T}, β::AbstractVector{T}) where {T}\n        out .= grad_enzyme(β, X, y)\n        return out\n    end\n    _grad!\nend\nfunction make_hess!(X, y)\n    function _hess!(out::AbstractMatrix{T}, β::AbstractVector{T}) where {T}\n        J, _ = jacobian_enzyme(β, X, y)\n        out .= J\n        return out\n    end\n    _hess!\nend\n\nlogpostE! = make_logpost!(X, y)\ngradE! = make_grad!(X, y)\nhessE! = make_hess!(X, y)\n\ninplace_enzyme = ExponentialFamilyProjection.InplaceLogpdfGradHess(logpostE!, gradE!, hessE!)\nprj_enzyme = ProjectedTo(MvNormalMeanCovariance, d; parameters = params)\nresult_enzyme = project_to(prj_enzyme, inplace_enzyme)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can quickly compare the runtime of the Enzyme-based implementation to the manual one defined above.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# 12) Speed comparison against the manual implementation from above\nt_manual = @belapsed project_to($prj, $inplace)\nt_enzyme = @belapsed project_to($prj_enzyme, $inplace_enzyme)\nspeedup = t_manual / t_enzyme\nround.((speedup, t_manual, t_enzyme); digits = 3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"On typical runs we observe a substantial speedup (often around 10×) for Enzyme while maintaining the same result.","category":"page"},{"location":"#Projection-with-samples","page":"Home","title":"Projection with samples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The projection can be done given a set of samples instead of the function directly. For example, let's project an set of samples onto a Beta distribution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StableRNGs\n\nhiddenbeta = Beta(10, 3)\nsamples = rand(StableRNG(42), hiddenbeta, 1_000)\nprj = ProjectedTo(Beta)\nresult = project_to(prj, samples)\n@test kldivergence(result, hiddenbeta) < 1e-2 #hide\nresult #hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(0.0:0.01:1.0, x -> pdf(hiddenbeta, x), label=\"real distribution\", fill = 0, fillalpha = 0.2)\nhistogram!(samples, label = \"samples\", normalize = :pdf, fillalpha = 0.2)\nplot!(0.0:0.01:1.0, x -> pdf(result, x), label=\"estimated projection\", fill = 0, fillalpha = 0.2)","category":"page"},{"location":"#Other","page":"Home","title":"Other","text":"","category":"section"},{"location":"#Manopt-extensions","page":"Home","title":"Manopt extensions","text":"","category":"section"},{"location":"#ExponentialFamilyProjection.ProjectionCostGradientObjective","page":"Home","title":"ExponentialFamilyProjection.ProjectionCostGradientObjective","text":"ProjectionCostGradientObjective\n\nThis structure provides an interface for Manopt to compute the cost and gradients required for the optimization procedure based on manifold projection. The actual computation of costs and gradients is defined by the strategy argument.\n\nArguments\n\nprojection_parameters: The parameters for projection, must be of type ProjectionParameters\nprojection_argument: The second argument of the project_to function.\ncurrent_η: Current optimization point.\nsupplementary_η: A tuple of additional natural parameters subtracted from the current point in each optimization iteration.\nstrategy: Specifies the method for computing costs and gradients, which may support different projection_argument values.\nstrategy_state: The state for the strategy, usually created with create_state!\n\nnote: Note\nThis structure is internal and is subject to change.\n\n\n\n\n\n","category":"type"},{"location":"#Bounded-direction-update-rule","page":"Home","title":"Bounded direction update rule","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ExponentialFamilyProjection.jl package implements a specialized gradient direction rule that limits the norm (manifold-specific) of the gradient to a pre-specified value.","category":"page"},{"location":"#ExponentialFamilyProjection.BoundedNormUpdateRule","page":"Home","title":"ExponentialFamilyProjection.BoundedNormUpdateRule","text":"BoundedNormUpdateRule(limit; direction = Manopt.IdentityUpdateRule())\n\nA DirectionUpdateRule is a direction rule that constrains the norm of the direction to a specified limit.\n\nThis rule operates in two steps:\n\nInitial direction computation: It first applies the specified direction update rule to compute an initial direction.\nNorm check and scaling: The norm of the resulting direction vector is checked using Manopt.norm(M, p, d), where:\nM` is the manifold on which the optimization is running,\np is the point at which the direction was computed,\nd is the computed direction.\nIf this norm exceeds the specified limit, the direction vector is scaled down so that its new norm exactly equals the limit. This scaling preserves the direction of the gradient while controlling its magnitude.\n\nRead more about Manopt.DirectionUpdateRule in the Manopt.jl documentation.\n\n\n\n\n\n","category":"type"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
